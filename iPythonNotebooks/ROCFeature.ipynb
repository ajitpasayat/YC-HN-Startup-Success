{
 "metadata": {
  "name": "",
  "signature": "sha256:8828903942040580502ca7c068191e9286ed5ed214af65158814a03e4e0eab60"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import feature_extraction\n",
      "\n",
      "# Borrowed from: http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/tree/master/ (Chapter 4)\n",
      "def one_hot_dataframe(data, cols, replace=False):\n",
      "    \"\"\" Takes a dataframe and a list of columns that need to be encoded.\n",
      "    Returns a 3-tuple comprising the data, the vectorized data,\n",
      "    and the fitted vectorizor.\n",
      "    Modified from https://gist.github.com/kljensen/5452382\n",
      "    \"\"\"\n",
      "    vec = feature_extraction.DictVectorizer()\n",
      "    mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
      "\n",
      "    vecData = pd.DataFrame(vec.fit_transform(data[cols].to_dict(outtype='records')).toarray())\n",
      "    vecData.columns = vec.get_feature_names()\n",
      "    vecData.index = data.index\n",
      "    if replace is True:\n",
      "        data = data.drop(cols, axis=1)\n",
      "        data = data.join(vecData)\n",
      "    return (data, vecData)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load HackerNews data.\n",
      "hacerknewsdata = pd.DataFrame()\n",
      "hackernewsdata = pd.read_csv('../Datasets/HNTitleCommentsSentimentData.csv')\n",
      "hackernewsdata = hackernewsdata.drop(['Unnamed: 0'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load YC startups data.\n",
      "cleanstartups = pd.DataFrame()\n",
      "cleanstartups = pd.read_csv('../Datasets/CleanStartupsFull.csv')\n",
      "cleanstartups = cleanstartups.drop(['Unnamed: 0'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Merge the cleaned-up YC startup data with Hacker News data.\n",
      "fullset = pd.merge(hackernewsdata, cleanstartups, on='Company', how='inner')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Going to convert those that have exited YC as remaining in operation.\n",
      "# I want to keep the perdiction simple: in operation or dead.\n",
      "# I will think of ways to incorporate startups that have exited as a classifer after my demo.\n",
      "fullset.Fate[fullset.Fate != 'Dead'] = 'Operating'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To create training set, first separate startups that are not operating versus those that remain in operation.\n",
      "# Because there aren't as many dead startups, I will only use an equal amount of active startups (chosen by random). \n",
      "deadset = fullset[fullset.Fate == 'Dead']\n",
      "operatingset = fullset[fullset.Fate == 'Operating']\n",
      "rows = np.random.choice(operatingset.index.values, len(deadset))\n",
      "operatingset = operatingset.ix[rows]\n",
      "\n",
      "# Now merge together the dead and operating startups -- this will be the training set for now.\n",
      "trainingset = deadset.append(operatingset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Truncating full set to exclude features that aren't going to be in the decision tree.\n",
      "trainingset = trainingset.drop(['Company','Title','TopComment','TopCommentPoints','Fate','Description','Logo',\n",
      "                    'SeedDBMattermarkProfile','CrunchBaseAngelListProfile','Website',\n",
      "                    'YCYear','YCSession','Market','Founders','Investors'], 1)\n",
      "\n",
      "# Also have to deal with missing values and data types.\n",
      "trainingset[['Sentiment','TotalFunds']] = trainingset[['Sentiment','TotalFunds']].fillna(0.0).astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testset = pd.DataFrame(fullset.Fate[trainingset.index])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import label_binarize\n",
      "# Transforming lists of feature-value mappings to vectors.\n",
      "# This also transforms and encodes categorical integer features using a variation of one-hot scheme.\n",
      "trainingset, trainingset_n = one_hot_dataframe(trainingset, ['City', 'Investors1', 'Investors2', 'Investors3', 'Investors4',\n",
      "                                                             'Founders1', 'Founders2', 'Founders3', 'Founders4',\n",
      "                                                             'Market1', 'Market2', 'Market3', 'Market4', \n",
      "                                                             'Market5'], replace=True)\n",
      "trainingset = trainingset.fillna(0)\n",
      "\n",
      "testset, testset_n = one_hot_dataframe(testset, ['Fate'], replace=True)\n",
      "testset = testset.fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Binarize the output\n",
      "testset = label_binarize(testset, classes=[0, 1])\n",
      "n_classes = testset.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(trainingset, \n",
      "                                                                     testset, \n",
      "                                                                     test_size=0.4)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.externals import joblib\n",
      "clf = joblib.load('../Dump/randomforeststartup.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_classification\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "importances = clf.feature_importances_\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "# Print the feature ranking\n",
      "print(\"Feature ranking:\")\n",
      "\n",
      "for f in range(10):\n",
      "    print(\"%d. %s (%f)\" % (f + 1, trainingset.columns[indices][f], importances[indices[f]]))\n",
      "\n",
      "# Plot the feature importances of the forest\n",
      "figure(figsize=(10, 7.5), dpi=600)\n",
      "barh(range(10), importances[indices][:10], color=\"orange\", align=\"center\")\n",
      "yticks(range(10), trainingset.columns[indices][:10])\n",
      "plt.ylim([-1,10])\n",
      "plt.gca().invert_yaxis()\n",
      "title(\"Feature Importances\")\n",
      "plt.savefig('Figures/FeatureImportance.png')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature ranking:\n"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "index 513 is out of bounds for size 504",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-19-8e5d41817681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d. %s (%f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Plot the feature importances of the forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index 513 is out of bounds for size 504"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
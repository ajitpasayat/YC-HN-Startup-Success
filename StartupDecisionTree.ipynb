{
 "metadata": {
  "name": "",
  "signature": "sha256:6069934efc99da358f9c64b582d7598bd996f388c1f7dde116a6467155400672"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import feature_extraction\n",
      "\n",
      "# Borrowed from: http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/tree/master/ (Chapter 4)\n",
      "def one_hot_dataframe(data, cols, replace=False):\n",
      "    \"\"\" Takes a dataframe and a list of columns that need to be encoded.\n",
      "    Returns a 3-tuple comprising the data, the vectorized data,\n",
      "    and the fitted vectorizor.\n",
      "    Modified from https://gist.github.com/kljensen/5452382\n",
      "    \"\"\"\n",
      "    vec = feature_extraction.DictVectorizer()\n",
      "    mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
      "\n",
      "    vecData = pd.DataFrame(vec.fit_transform(data[cols].to_dict(outtype='records')).toarray())\n",
      "    vecData.columns = vec.get_feature_names()\n",
      "    vecData.index = data.index\n",
      "    if replace is True:\n",
      "        data = data.drop(cols, axis=1)\n",
      "        data = data.join(vecData)\n",
      "    return (data, vecData)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load HackerNews data.\n",
      "hacerknewsdata = pd.DataFrame()\n",
      "hackernewsdata = pd.read_csv('HNTitleCommentsSentimentData.csv')\n",
      "hackernewsdata = hackernewsdata.drop(['Unnamed: 0'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load YC startups data.\n",
      "cleanstartups = pd.DataFrame()\n",
      "cleanstartups = pd.read_csv('CleanStartups.csv')\n",
      "cleanstartups = cleanstartups.drop(['Unnamed: 0'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Merge the cleaned-up YC startup data with Hacker News data.\n",
      "fullset = pd.merge(hackernewsdata, cleanstartups, on='Company', how='inner')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Going to convert those that have exited YC as remaining in operation.\n",
      "# I want to keep the perdiction simple: in operation or dead.\n",
      "# I will think of ways to incorporate startups that have exited as a classifer after my demo.\n",
      "fullset.Fate[fullset.Fate != 'Dead'] = 'Operating'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To create training set, first separate startups that are not operating versus those that remain in operation.\n",
      "# Because there aren't as many dead startups, I will only use an equal amount of active startups (chosen by random). \n",
      "deadset = fullset[fullset.Fate == 'Dead']\n",
      "operatingset = fullset[fullset.Fate == 'Operating']\n",
      "rows = np.random.choice(operatingset.index.values, len(deadset))\n",
      "operatingset = operatingset.ix[rows]\n",
      "\n",
      "# Now merge together the dead and operating startups -- this will be the training set for now.\n",
      "trainingset = deadset.append(operatingset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Truncating full set to exclude features that aren't going to be in the decision tree.\n",
      "trainingset = trainingset.drop(['Company','Title','TopComment','Fate','Description','Logo',\n",
      "                    'SeedDBMattermarkProfile', 'CrunchBaseAngelListProfile', 'Website'], 1)\n",
      "\n",
      "# Also have to deal with missing values and data types.\n",
      "trainingset[['Sentiment','TotalFunds']] = trainingset[['Sentiment','TotalFunds']].fillna(0.0).astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testset = pd.DataFrame(fullset.Fate[trainingset.index])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transforming lists of feature-value mappings to vectors.\n",
      "# This also transforms and encodes categorical integer features using a variation of one-hot scheme.\n",
      "trainingset, trainingset_n = one_hot_dataframe(trainingset, ['YCSession', 'City',\n",
      "                                                             'Investors1', 'Investors2', 'Investors3', 'Investors4',\n",
      "                                                             'Founders1', 'Founders2', 'Founders3', 'Founders4',\n",
      "                                                             'Market1', 'Market2', 'Market3', 'Market4', \n",
      "                                                             'Market5'], replace=True)\n",
      "trainingset = trainingset.fillna(0)\n",
      "\n",
      "testset, testset_n = one_hot_dataframe(testset, ['Fate'], replace=True)\n",
      "testset = testset.fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(trainingset, \n",
      "                                                                     testset, \n",
      "                                                                     test_size=0.4, \n",
      "                                                                     random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.84210526315789469"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save the model \n",
      "import pickle\n",
      "pickle.dump( clf, open( \"startupdtree.p\", \"wb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    }
   ],
   "metadata": {}
  }
 ]
}